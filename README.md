# bert-cuad

### Bert on CUAD using Jupyter Notebooks

![bert](https://user-images.githubusercontent.com/27162948/172364648-4587b7b2-7d37-4429-aa48-565d2049e5ab.jpeg)

  BERT stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.

  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). 

  This repository contains 3 jupyter notebook. The notebooks contains the steps to download the original repository from the Atticus Project, the steps to download the Bert model and a walk through to install the necessary dependencies. Please run the notebooks in order starting with 1_preparation.jpynb

Notebook 3 contains an implementation of Tesseract OCR. With Tesseract is possible to extract the text from an image to later feed the model with that text.

**Project:** 

https://www.atticusprojectai.org/cuad

**Git Hub of the project:** 

https://github.com/TheAtticusProject/cuad

**CUAD Dataset:** 

https://huggingface.co/datasets/cuad

*Bert models are under Apache License 2.0*

** ** 
**Benetti Mauro 05.2022**

**License: CC BY 4.0** 

###### https://creativecommons.org/licenses/by/4.0/
